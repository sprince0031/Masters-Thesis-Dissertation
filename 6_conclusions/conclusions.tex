% this file is called up by thesis.tex
% content in this file will be fed into the main document

\chapter{Conclusion and Future Directions } % top level followed by section, subsection


% ----------------------- paths to graphics ------------------------

% change according to folder and file names
\ifpdf
    \graphicspath{{6_conclusions/figures/PNG/}{6_conclusions/figures/PDF/}{6_conclusions/figures/}}
\else
    \graphicspath{{6_conclusions/figures/EPS/}{6_conclusions/figures/}}
\fi


% ----------------------- contents from here ------------------------

\section{Future Work}

With future directions in which research can be taken, probably the most obvious one would be to simply continue training the model for significantly more number of epochs. The second intuitive choice would be to now train the fine-tuned SD locked model without prompt input so that it could potentially learn to distinguish between classes with just the H\&E image inputs. A good idea to gradually help the model learn these mappings would be to perform gradual masking of the text prompts over time until no prompts are used. This could theoretically help the model converge faster than if the prompts were just disabled altogether. 

Other more experimental approaches would be to try different combinations of the ideas proposed by latest research in this field such as adding discriminator blocks from the GAN architecture to the Stable Diffusion pipeline \parencite{Wang2022Diffusion-GAN:Diffusion}, using better optimised loss functions for training such as in Pyramid pix2pix \parencite{Liu2022BCI:Pix2pix}
or the method proposed by \textcite{Ma2024DSFF-GAN:Cancer} with DSFF-GAN.

Some good data preprocessing that can be done to improve results would be to normalise image colours before training the model. Having a lot of variability in the image properties such as image contrast and colour intensity will lead to lesser accurate learning as was the case in this thesis. Also, always finding varied data sources will improve the robustness of learning and not make it overfit on just one dataset. However, this has never been a very accessible task when it comes to medical data.

A final idea to take this project forward could be to explore reinforcement learning techniques with the ControlNet architecture for a different approach to handling loss during training.

\section{Conclusions}

Overall this body of research aimed to explore the application and efficacy of a new and cutting-edge generative model. Of the three research questions posed, the first two have been explored where the results showed a definite degree of morphological parallels between the H\&E and HER2 IHC patches. For the second research question of training this model to perform better than the current state-of-the-art models, it did not achieve that but given that. However, the qualitative analysis, though the result comparison not perfect, being on par with the research this thesis was based upon is a definite positive. The final research question of training the SD model to act as a classifier while also generating IHC images was not able to be realised and is the biggest limitation. The latest literature was reviewed and many decisions in implementation was made based off this. The results were analysed and discussed in a way that would make sense for the nature of generative image research and evaluation metrics followed by other research in this field was used.

The stain transfer area of research though niche, is promising with many cutting-edge generative techniques being researched to improve the quality-of-life for pathologists and cancer patients alike. Discovering and inventing better technology to fight and contribute to the fight against cancer is always a noble pursuit. Demonstrating the capabilities of the model in this thesis with less training cycles proves that this pursuit is worth pursuing. 


% ---------------------------------------------------------------------------
% ----------------------- end of thesis sub-document ------------------------
% ---------------------------------------------------------------------------